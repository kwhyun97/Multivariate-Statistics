---
title: "Assignment 7"
subtitle: "STAT 32950"
author: "Ki Hyun"
date: "Due: 09:00 (CT) 2023-05-09"
output: pdf_document
---

```{r packages, message = FALSE, warning = FALSE}
library(stats)
library(dplyr)
library(ggplot2)
library(mclust)
library(mgcv)
library(rlang)
```


# Problem 1.

```{r q1}
Ch1_2 = 0.76; Ch1_3 = 2.97; Ch1_4 = 4.88; Ch1_5 = 3.86
Ch2_3 = 0.80; Ch2_4 = 4.17; Ch2_5 = 1.96
Ch3_4 = 0.21; Ch3_5 = 1.51
Ch4_5 = 0.51
q1_data <- tibble("Ch1" = c(0, Ch1_2, Ch1_3, Ch1_4, Ch1_5),
                  "Ch2" = c(Ch1_2, 0, Ch2_3, Ch2_4, Ch2_5),
                  "Ch3" = c(Ch1_3, Ch2_3, 0, Ch3_4, Ch3_5),
                  "Ch4" = c(Ch1_4, Ch2_4, Ch3_4, 0, Ch4_5),
                  "Ch5" = c(Ch1_5, Ch2_5, Ch3_5, Ch4_5, 0))
q1_dmat <- as.dist(q1_data)
```

## (a)

```{r q1_a_single, eval = F}
Msingle = hclust(q1_dmat, method = "single")
plot(Msingle)
```

```{r q1_a_complete, eval = F}
Mcomplete = hclust(q1_dmat, method = "complete")
plot(Mcomplete)
```

```{r q1_a_average, eval = F}
Maverage = hclust(q1_dmat, method = "average")
plot(Maverage)
```

The `complete` and the `average` methods result in a similar dendrogram. 
However, the vertical scales are different. The `single` method has a different
cluster dendrogram and the smallest vertical scale.

## (b)

- Single Method: (Ch1, Ch2), Ch5, (Ch3, Ch4)

- Complete Method: (Ch3, Ch4), (Ch1, Ch2), Ch5

- Average Method: (Ch3, Ch4), (Ch1, Ch2), Ch5

\newpage

# Problem 2.

```{r q2}
X1 = c(5, 1, -1, 3)
X2 = c(-4, -2, 1, 1)
A = c(X1[1], X2[1])
B = c(X1[2], X2[2])
C = c(X1[3], X2[3])
D = c(X1[4], X2[4])
```

```{r q2_algo}
get_dist <- function(obj1, obj2){
  if(length(obj1) != length(obj2)){
    print("Dimensions do not match")
    return(NULL)
  }
  return(sqrt(sum((obj1 - obj2)^2)))
}

get_center <- function(objects){
  n <- length(objects)
  centroid <- 0
  for(obj in objects){
    centroid = centroid + obj
  }
  centroid/n
}

# since k = 2 there are two groups
groups = list(list(A, B), list(C, D)) # initial groups

counter <- 0
deletion <- c()
centroid = get_center(groups[[1]])
this_group = groups[[1]]
other_group = groups[[2]]
for(i in 1:length(this_group)){
  member = this_group[[i]]
  current_d <- get_dist(member, centroid)
  temp_cent = get_center(append(other_group, member))
  other_d <- get_dist(member, temp_cent)
  if(other_d < current_d){
    other_group = append(other_group, member)
    deletion <- c(deletion, i)
    counter = counter + 1
  }
}
if(length(deletion) > 0){
  groups[[1]] = this_group[-deletion]
}
groups[[2]] = other_group

deletion <- c()
centroid = get_center(groups[[2]])
this_group = groups[[2]]
other_group = groups[[1]]
for(i in 1:length(this_group)){
  member = this_group[[i]]
  current_d <- get_dist(member, centroid)
  temp_cent = get_center(append(this_group, member))
  other_d <- get_dist(member, temp_cent)
  if(other_d < current_d){
    other_group = append(other_group, member)
    deletion <- c(deletion, i)
    counter = counter + 1
  }
}
if(length(deletion) > 0){
  groups[[2]] = this_group[-deletion]
}
groups[[1]] = other_group
```

The final clusters are (B, C, D), A

The corresponding centroids are (x1 = 1, x2 = 0) and (x1 = 5, x2 = -4)

The squared sitances to cluster centroids are:

A - Group 1: 32, Group 2: 0
B - Group 1: 4, Group 2: 20
C - Group 1: 5, Group 2: 61
D - Group 1: 5, Group 2: 29

\newpage

# Problem 3.

```{r q3}
ladyrun = read.table("ladyrun23.dat")
colnames(ladyrun)=c("Country","100m","200m","400m","800m","1500m","3000m",
                    "Marathon")
```

## (a)

```{r q3_a}
dist_m <- tibble("Null" = rep(0, 54))
for(i in 1:nrow(ladyrun)){
  country <- ladyrun$Country[i]
  temp <- c()
  for(j in 1:nrow(ladyrun)){
    temp <- c(temp, get_dist(ladyrun[i, -1], ladyrun[j, -1]))
  }
  dist_m[country] = temp
}
dist_m <- dist_m[-1]
```

```{r q3_a_max}
max_countries <- which(dist_m == max(as.dist(dist_m))) %/% nrow(ladyrun)
max_countries[2] = max_countries[2] + 1
print(paste0(ladyrun$Country[max_countries[1]],
             " and ",
             ladyrun$Country[max_countries[2]]))
```

Japan and Papua New Guinea have the maximum distance

```{r q3_a_min}
min_countries <- which(dist_m == min(as.dist(dist_m))) %/% nrow(ladyrun)
min_countries[2] = min_countries[2] + 1
print(paste0(ladyrun$Country[min_countries[1]],
             " and ",
             ladyrun$Country[min_countries[2]]))
```

Brazil and Spain have the minimum distance.

## (b)

```{r q3_b_single}
Msingle_2 = hclust(as.dist(dist_m), method = "single")
plot(Msingle_2)
```

```{r q3_b_complete}
Mcomplete_2 = hclust(as.dist(dist_m), method = "complete")
plot(Mcomplete_2)
```

The structure and the vertical scale of the two dendrograms are different.
However, the two are similar in the fact that ("COK", "PNG") and "SAM" are
clustered different to other countries.

When $k = 8$ or $7$, the three smallest clusters are "SAM", ("COK", "PNG"), and
("GBR", "KEN").

## (c)

```{r q3_c}
kmeans(ladyrun[-1], 9)
```

I would choose $k = 9$.

```{r q3_c_plot}
M0_q3 <- kmeans(ladyrun[-1], 9)
PC <- princomp(ladyrun[-1], cor = T)

tibble(x = PC$scores[,1],
       y = PC$scores[,2],
       groups_lab = as.character(M0_q3$cluster)) %>% 
  ggplot(mapping = aes(x = x, y = y, color = groups_lab)) +
  geom_point() +
  scale_color_discrete() +
  labs(x = "First Principal Component",
       y = "Second Principal Component",
       color = "Clusters",
       title = "Lady run Data k-means, k = 9") +
  theme_bw(base_size = 13)
```

\newpage

# Problem 4.

## (a)

$$
\begin{aligned}
& f_1(0.1) = 0.2, \ f_2(0.1) = 1.8 \\
& f_1(0.2) = 0.4, \ f_2(0.2) = 1.6 \\
& f_1(0.3) = 0.6, \ f_2(0.1) = 1.4 \\
& f_1(0.4) = 0.8, \ f_2(0.4) = 1.2 \\
& f_1(0.7) = 1.4, \ f_2(0.7) = 0.6 
\end{aligned}
$$

The likelihood function becomes:

$$
\begin{aligned}
& \prod_{i = 1}^5
(p_1 f_1(x_i) + p_2 f_2(x_i)) \\
=& (0.2 p_1 + 1.8 p_2) \cdot
(0.4 p_1 + 1.6 p_2) \cdot
(0.6 p_1 + 1.4 p_2) \cdot
(0.8 p_1 + 1.2 p_2) \cdot
(1.4 p_1 + 0.6 p_2)
\end{aligned}
$$

## (b)

$$
\begin{aligned}
& f_1(0.1) = 0.2, \ f_2(0.1) = 1.8 \\
& f_1(0.2) = 0.4, \ f_2(0.2) = 1.6 \\
& f_1(0.3) = 0.6, \ f_2(0.1) = 1.4 \\
& f_1(0.4) = 0.8, \ f_2(0.4) = 1.2 \\
& f_1(0.9) = 1.8, \ f_2(0.9) = 0.2 
\end{aligned}
$$

The likelihood function becomes:

$$
\begin{aligned}
& \prod_{i = 1}^5
(p_1 f_1(x_i) + p_2 f_2(x_i)) \\
=& (0.2 p_1 + 1.8 p_2) \cdot
(0.4 p_1 + 1.6 p_2) \cdot
(0.6 p_1 + 1.4 p_2) \cdot
(0.8 p_1 + 1.2 p_2) \cdot
(1.8 p_1 + 0.2 p_2)
\end{aligned}
$$

## (c)

$$
\begin{aligned}
& f_1(0.1) = 0.2, \ f_2(0.1) = 1.8 \\
& f_1(0.2) = 0.4, \ f_2(0.2) = 1.6 \\
& f_1(0.3) = 0.6, \ f_2(0.1) = 1.4 \\
& f_1(0.6) = 1.2, \ f_2(0.6) = 0.8 \\
& f_1(0.9) = 1.8, \ f_2(0.9) = 0.2 
\end{aligned}
$$

The likelihood function becomes:

$$
\begin{aligned}
& \prod_{i = 1}^5
(p_1 f_1(x_i) + p_2 f_2(x_i)) \\
=& (0.2 p_1 + 1.8 p_2) \cdot
(0.4 p_1 + 1.6 p_2) \cdot
(0.6 p_1 + 1.4 p_2) \cdot
(1.2 p_1 + 0.8 p_2) \cdot
(1.8 p_1 + 0.2 p_2)
\end{aligned}
$$

## (d)

```{r q3_d_f}
q3_log_likelihood <- function(p, xs){
  ret = 0
  for(constant in xs){
    ret = ret + log(2*constant * p + (2 - 2*constant) * (1 - p))
  }
  ret
}
```

```{r q3_d_plot}
p <- seq(0, 1, by = 10^(-4))
y1 = q3_log_likelihood(p, c(0.1, 0.2, 0.3, 0.4, 0.7))
y2 = q3_log_likelihood(p, c(0.1, 0.2, 0.3, 0.4, 0.9))
y3 = q3_log_likelihood(p, c(0.1, 0.2, 0.3, 0.6, 0.9))

ggplot() +
  geom_line(mapping = aes(x, y, col = "a"),
            data = tibble(x = p,
                          y = y1)) +
  geom_vline(xintercept = p[y1 == max(y1)], 
             color = "red", linetype = "dashed") +
  geom_line(mapping = aes(x, y, col = "b"),
            data = tibble(x = p,
                          y = y2)) +
  geom_vline(xintercept = p[y2 == max(y2)], 
             color = "blue", linetype = "dashed") +
  geom_line(mapping = aes(x, y, col = "c"),
            data = tibble(x = p,
                          y = y3)) +
  geom_vline(xintercept = p[y3 == max(y3)], 
             color = "green", linetype = "dashed") +
  labs(x = "p1", y = "Log-likelihood", color = "Legened") +
  scale_color_manual(values = c("a" = "red", "b" = "blue", "c" = "green")) +
  theme_bw(base_size = 13)
```

## (e)

For (a), $(\hat{p}_1, \hat{p}_2)$ can be estimated as $(0, 1)$. 

For (b), $(\hat{p}_1, \hat{p}_2)$ can be estimated as $(0.21, 0.79)$. 

For (c), $(\hat{p}_1, \hat{p}_2)$ can be estimated as $(0.29, 0.71)$. 

The estimates seems reasonable except for the case of (a)

\newpage

# Problem 5.

```{r q5}
heart=read.table("heart.dat")
colnames(heart)=c("age","sex","chest","bp","chl","sugar","ecg","rate","angina",
                  "peak","slope","vssl","thal","ill")
```

```{r q5_plot}
heart_real = heart[c(1, 4, 5, 8, 10, 12)]
pairs(heart_real)
```

Now using BIC to choose the number of clusters:

```{r q5_cluster}
mheart = Mclust(heart_real)
summary(mheart)
```

The chosen number of clusters is $k = 5$

Now looking at BIC plot for model selection:

```{r q5_BIC_plot}
plot(mheart, what = c("BIC"))
```

Now looking at the best candidates of model assumptions by BIC:

```{r q5_model_assumption}
summary(mclustBIC(heart_real))
```

Now fixing the $k$ to be $5$,

```{r classification_plot}
heart5 = Mclust(heart_real, G = 5)
plot(heart5, what = c("classification"))
```

```{r uncertainty_plot}
plot(heart5, what = c("uncertainty"))
```

```{r density_plot}
plot(heart5, what = c("density"))
```

Looking at the model assumptions for $k = 5$ mixtures:

```{r mixtures_summary}
summary(mclustBIC(heart_real, G = 5))
```

Finally looking at the mixture proportions:

```{r mixture_proportions}
heart5$parameters$pro
```

\newpage

# Problem 6.

## (a)

Using averages of each column to impute initial estimates:

$$
\tilde{\mathbf{X}} = \begin{bmatrix}
3 & 6 & 0 \\
4 & 4 & 3 \\
4 & 8 & 3 \\
5 & 6 & 2
\end{bmatrix}
$$

## (b)

First, the mean can be estimated from the above as:

$$
\tilde{\mathbf{\mu}} = \begin{bmatrix}
4 \\ 6 \\ 2
\end{bmatrix}
$$

The maximum likelihood covariance matrix can be estimated as:

```{r q6_b}
X_t <- matrix(c(3, 6, 0, 4, 4, 3, 4, 8, 3, 5, 6, 2), ncol = 3, byrow = T)
mu = colMeans(X_t)
Sigma = cov(X_t) * ((nrow(X_t) - 1)/nrow(X_t))
Sigma
```

$$
\tilde{\Sigma} \approx \begin{bmatrix}
0.5 & 0.0 & 0.5 \\
0.0 & 2.0 & 0.0 \\
0.5 & 0.0 & 1.5
\end{bmatrix}
$$

## (c)

### i)

Using the estimated $\tilde{\mathbf{\mu}}$ and $\tilde{\Sigma}$, we may 
update $\tilde{x}_{13}$ with the expected value of the $X_1 \mid X_2 = x_2$ 
conditional distribution as:

$$
\begin{aligned}
& \tilde{\mu}_1 + 
\begin{bmatrix}
0.0 & 0.5
\end{bmatrix} \times
\begin{bmatrix}
2.0 & 0.0 \\ 0.0 & 1.5
\end{bmatrix}^{-1} \times
\left(
\begin{pmatrix}
8 \\ 3
\end{pmatrix} - 
\begin{pmatrix}
6 \\ 2
\end{pmatrix}
\right) \\
=& 4 +
\begin{bmatrix}
0.0 & 0.5
\end{bmatrix} \times
\begin{bmatrix}
0.5 & 0.0 \\ 0.0 & \frac{2}{3}
\end{bmatrix} \times
\begin{pmatrix}
2 \\ 1
\end{pmatrix} \\
=& 4 + 
\begin{bmatrix}
0.0 & \frac{1}{3}
\end{bmatrix} \times
\begin{bmatrix}
2 \\ 1
\end{bmatrix} \\
\approx & 4.33
\end{aligned}
$$
Therefore, the updated $\tilde{\mathbf{X}}'$ is:

$$
\tilde{\mathbf{X}}' \approx \begin{bmatrix}
3 & 6 & 0 \\
4 & 4 & 3 \\
4.33 & 8 & 3 \\
5 & 6 & 2
\end{bmatrix}
$$

```{r q6_c_i}
X_t_p <- matrix(c(3, 6, 0, 4, 4, 3, 13/3, 8, 3, 5, 6, 2), ncol = 3, byrow = T)
mu_p = colMeans(X_t_p)
Sigma_p = cov(X_t_p) * ((nrow(X_t_p) - 1)/nrow(X_t_p))
Sigma_p
```

Now the updated estimates of $\tilde{\mathbf{\mu}}$ and $\tilde{\Sigma}$ are:

$$
\tilde{\mathbf{\mu}}' = \begin{bmatrix}
4.083 \\ 6 \\ 2
\end{bmatrix}
$$

$$
\tilde{\Sigma}' \approx \begin{bmatrix}
0.521 & 0.167 & 0.583 \\
0.167 & 2.0 & 0.0 \\
0.583 & 0.0 & 1.5
\end{bmatrix}
$$

### ii)

```{r q6_c_ii}
update_23 <- mu[2:3] + 
  c(Sigma_p[1, 2], Sigma_p[1, 3])%*%solve(Sigma_p[2:3, 2:3]) * (5 - mu_p[1])
update_23
```

Therefore, the updated $\tilde{\mathbf{X}}''$ is:

$$
\tilde{\mathbf{X}}'' \approx \begin{bmatrix}
3 & 6 & 0 \\
4 & 4 & 3 \\
4.33 & 8 & 3 \\
5 & 6.08 & 2.36
\end{bmatrix}
$$

```{r q6_c_last_update}
X_t_pp <- matrix(c(3, 6, 0, 4, 4, 3, 13/3, 8, 3, 5, update_23[1], update_23[2]), 
                 ncol = 3, byrow = T)
mu_pp = colMeans(X_t_pp)
Sigma_pp = cov(X_t_pp) * ((nrow(X_t_pp) - 1)/nrow(X_t_pp))
Sigma_pp
```

Now the updated estimates, after the first iteration, of $\tilde{\mathbf{\mu}}$ 
and $\tilde{\Sigma}$ are:

$$
\tilde{\mathbf{\mu}}'' = \begin{bmatrix}
4.083 \\ 6.02 \\ 2.09
\end{bmatrix}
$$

$$
\tilde{\Sigma}'' \approx \begin{bmatrix}
0.521 & 0.184 & 0.665 \\
0.184 & 2.00 & 0.00511 \\
0.665 & 0.00511 & 1.52
\end{bmatrix}
$$

\newpage

# Problem 7.

## (a)

Given that $a \in \mathbf{R}^p$ and $b \in \mathbf{R}^p$,
$(a - b) \in \mathbf{R}^p$. Therefore, we may let:

$$
a - b = 
\begin{pmatrix}
x_1 \\ \vdots \\ x_p
\end{pmatrix}
$$

where $x_i \in \mathbf{R}$. Moreover,

$$
(a - b)^T (a - b) =
\sum_{i = 1}^p x^2_i
\geq 0
$$

In other words $d(a, b) \geq 0$ for all $a$ and $b$.

Now, given that $d(a, b) - d(c, d) \geq 0$,

$$
\begin{aligned}
& d(a, b) - d(c, d) \geq 0 \\
\Leftrightarrow &
(d(a, b) + d(c, d))(d(a, b) - d(c, d)) \geq 0 \\
\Leftrightarrow &
d^2(a, b) - d^2(c, d) \geq 0 \\
\Leftrightarrow &
D(a, b) - D(c, d) \geq 0
\end{aligned}
$$

Therefore, the two dissimilarity measures are global-order equivalent.

## (b)

Using a simple example if we let

$$
\begin{aligned}
a &= \begin{pmatrix}
1 \\ -1
\end{pmatrix} \\
b &= \begin{pmatrix}
0 \\ 0
\end{pmatrix} \\
c &= \begin{pmatrix}
1 \\ 1
\end{pmatrix}
\end{aligned}
$$

Now,

$$
d_1(a , b) = \sqrt{(1 - 0)^2 + (-1 - 0)^2} = \sqrt{2}
$$

$$
d_1(a , c) = \sqrt{(1 - 1)^2 + (-1 - 1)^2} = 2
$$

We are able to see that $d_1(a , c) > d_1(a, b)$.

However, if we look at the city-block metric:

$$
d_2(a, b) = \frac{|1 - 0|}{|1| + |0|} +
\frac{|-1 - 0|}{|-1| + |0|} = 2
$$

$$
d_2(a, c) = \frac{|1 - 1|}{|1| + |1|} +
\frac{|-1 - 1|}{|-1| + |1|} = 1
$$

Therefore, in this case, $d_1(a , b) > d_1(a, c)$ and the above serves as 
a counter example.